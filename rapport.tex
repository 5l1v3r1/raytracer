\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[hyphens]{url} % à laisser avant hyperref
\usepackage[bookmarks,hidelinks]{hyperref}
\usepackage[french]{babel}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{afterpage}
\usepackage{listings}
\usepackage{lmodern}

\title{Rapport du projet de Technologies Objets}
\author{Maxime Arthaud \and Korantin Auguste \and Martin Carton}

\date{11 juin 2013}
\begin{document}
\maketitle 
\tableofcontents
\listoffigures
\newpage

\section{Introduction}
  Ce projet consiste en la création d'une interface utilisateur permettant de
  gérer une scène 3D et à la réalisation d'un moteur de rendu par lancé de
  rayons.
  Comme expliqué dans le rapport d'analyse, nous avons décidé de nous découper
  le travail, de façon à ce qu'une personne fasse l'interface graphique,
  une autre le parseur de fichier et l'écriture d'images au format PPM, et une
  autre travaille particulièrement sur le cœur du raytracer.

\section{Architecture}
  Le projet est constitué de deux programmes distincts:
  \begin{description}
      \item[L'interface graphique] qui devra permettre de créer simplement des
        fichiers représentant des scènes, à passer à notre raytracer;
      \item[Le raytracer] qui, à partir d'un fichier représentant une scène,
        devra générer un rendu, dans le format d'image de notre choix.
  \end{description}

  \subsection{Écriture d'images PPM}
    Pour écrire les images PPM, nous avons écrit un plugin Java qui permet
    d'enregistrer ce format auprès de \verb+javax.image.ImageIO+ qui propose une
    interface commune pour enregistrer des images dans différents formats. Ainsi
    notre programme peut générer des images au format PPM, mais aussi par
    exemple en PNG.
    
    Le choix de ce format est déterminé par le troisième paramètre d'appel du
    programme (la valeur pas défaut est png, ce format étant plus commun).
    Ce plugin consiste en deux classes \verb+imageio.PPMImageWriterSpi+ et
    \verb+imageio.PPMImageWriterSpi+. La première permet d'indiquer à Java les
    capacité de notre plugin. La deuxième permet d'écrire une image sur un flux
    de sortie quelconque.

  \subsection{Format de fichier}
    Nous avons décidé de mettre à disposition un format de fichier dans lequel
    on peut décrire une scène afin de pouvoir enregistrer celles-ci, ou de la
    générer automatiquement (ce qui nous a même permis de faire quelques
    vidéos).

    Le format de fichier ressemble à ceci:
    \begin{lstlisting}[caption=Exemple de fichier associé à la figure
    \ref{fig:rendusimple}]
//exemple de scene :
Camera(eye=(0, 0, 0), origin=(-0.5, 0, 1.5), abscissa=(1, 0, 0),  \
ordinate=(0, 1, 0), widthPixel=200, heightPixel=200)
AmbientLights(0.2, 0.2, 0.2)
Light(pos=(5, 0, 0), intensity=(0.5, 0.5, 0.5))
Sphere(center=(0, 5, 20), radius=3.14, k_diffuse=(0.6,0.3,0.6))
Plane(p1=(-1,0,30), p2=(1, 0, 30), p3=(0, 1, 30))
    \end{lstlisting}

    \begin{figure}[h]
      \centering{
        \includegraphics[width=200px]
        {livrables/tests/raytracer/fichier_simple.png}
      }
      \caption{Exemple de rendu simple\label{fig:rendusimple}}
    \end{figure}

    %todo: Korantin veut un fichier casse-sensible
    Il y a un élément par ligne et seule la ligne décrivant au moins une caméra
    est obligatoire. La casse et l'espacement sont ignorés. La plupart des
    propriétés ont une valeur par défaut afin d'éviter de surcharger le
    fichier et de permettre de l'écrire à la main. Des commentaires de fin de
    ligne commençant par «~\verb+//+~» peuvent être insérés.

    Les éléments suivants peuvent être ajoutés:
    \begin{description}
      \item[Camera] décrit une caméra de la scène, elle doit posséder les
        propriétés suivantes:
        \begin{description}
          \item[eye] un point décrivant la position de l'œil;
          \item[origin] un point décrivant l'origine du rectangle de l'écran;
          \item[abscissa et ordinate] les vecteurs qui avec \textit{origin}
            définissent l'écran;
          \item[widthPixel et heightPixem] deux entiers donnant la taille de
            l'image à générer. 
        \end{description}
      \item[AmbientLights] donne les trois composantes primaire de la lumière
        ambiante.
      \item[Light] décrit une source de lumière, elle doit avoir les propriétés
        suivantes:
        \begin{description}
          \item[pos] la position de cette source;
          \item[intensity] l'intensité lumineuse de chaque couleur primaire.
        \end{description}
      \item[Sphere] décrit une sphère, doit avoir les propriétés suivantes:
        \begin{description}
          \item[center] un point représentant le centre de la sphère;
          \item[radius] un flottant représentant le rayon de la sphère.
        \end{description}
      \item[Plane] décrit un plan, doit avoir trois points nommés \textit{P1},
        \textit{P2} et \textit{P3}.
      \item[Triangle] décrit un triangle, doit avoir trois points nommés
        \textit{P1}, \textit{P2} et \textit{P3}.
      \item[Parallelogram] décrit un parallélogramme, doit avoir trois points
        nommés \textit{P1}, \textit{P2} et \textit{P3}.
      \item[Cube] décrit un cube\footnote{En fait les points ne sont pas testés
        pour former un cube, cet objet peut représenter n'importe quel
        parallélépipède.}, doit avoir quatre points nommés
        \textit{P1}, \textit{P2}, \textit{P3} et \textit{P4} répartis comme
        ceci: \begin{lstlisting}
   p2------+
  /|      /|
 / |     / |
p1------p3 |
|  +----|--+
| /     | /
|/      |/
p4------+
        \end{lstlisting}
    \end{description}

    \bigskip
    Les objets \textit{Sphere}, \textit{Plane}, \textit{Triangle},
    \textit{Parallelogram} et \textit{Cube} peuvent en plus avoir les
    propriétés optionnelles suivantes:
    \begin{description}
      \item[brightness] flottant, par défaut à 30. Plus ce nombre est grand, plus la tâche provoquée par la composante spéculaire sera localisée.
      \item[k\_specular] flottant, par défaut à 1. Coefficient par lequel on multiplie la composante spéculaire.
      \item[k\_diffuse] triplet, par défaut à \textit{(1, 1, 1)}. Coefficients
        par lesquels on multiplie les composantes diffuse pour chaque couleur.
        Donne donc la couleur de l'objet.
      \item[k\_reflection] flottant, par défaut à 0. Coefficient par lequel on
        multiplie la composante réfléchie.
      \item[k\_refraction] triplet, par défaut à \textit{(0, 0, 0)}.
        Coefficients par lesquels on multiplie les composantes réfractées pour
        chaque couleur.
      \item[refractive\_index] flottant, par défaut à 1 (la lumière réfractée ne
        sera alors pas déviée). Indice du milieu intérieur à l'objet.
        % todo cela nécessite que l'on parle de l'orientation, notamment pour
        % les TriPoints
    \end{description}

    \bigskip
    Un objet de type \verb+Scene+ est construit à partir d'un tel fichier à
    l'aide la méthode statique \verb+raytracer.FileReader.read+.

  \subsection{Interface graphique}
    Notre interface graphique permet de manipuler simplement un fichier de scène, et peut aussi appeller simplement le moteur de raytracing pour faire un rendu.
    Une classe principale, \verb+Gui+, gére la fenêtre ainsi que l'emplacement des éléments.
    Elle est composée d'une partie supérieur contenant des onglets, une petite ligne contenant les boutons et enfin la zone de texte qui permet d'éditer la scène.
    Un onglet est représenté par une classe, qui hérite de \verb+Tab+, et permet d'ajouter un objet à la scène.
    Chaque onglet a une liste de champs, qui servent à paramètrer l'objet placé dans la scène (comme sa position ou sa brillance).
    Ces champs sont contenus dans une liste de type \verb+List<TabField>+,  qui est geré par la classe Tab.
    Ainsi, pour ajouter un onglets, il suffit de créer une classe qui hérite de \verb+Tab+, et qui ajoute des instances de \verb+IntegerTabField+, \verb+DoubleTabField+ ou \verb+Point3dTabField+.
    Les boutons au centre permettent d'enregistrer, de voir l'image actuelle ou d'ajouter un objet.
    
    Le bouton «~Voir l'image~» fait appel à la classe \verb+GenerateImageAction+, qui importe le moteur de raytracing, et crée les images dans un autre thread, pour que l'interface ne soit pas bloquée.
    Elles sont alors affichées avec des JFrames, dans un JLabel.
    
    Une classe principale, \verb+Gui+, gére la fenêtre ainsi que l'emplacement des éléments.
    Elle est composée d'une partie supérieur contenant des onglets, une petite ligne contenant les boutons et enfin la zone de texte qui permet d'éditer la scène.
    Un onglet est représenté par une classe, qui hérite de \verb+Tab+, et permet d'ajouter un objet à la scène.
    Chaque onglet a une liste de champs, qui servent à paramètrer l'objet placé dans la scène (comme sa position ou sa brillance).
    Ces champs sont contenus dans une liste de type \verb+List<TabField>+,  qui est geré par la classe Tab.
    Ainsi, pour ajouter un onglets, il suffit de créer une classe qui hérite de \verb+Tab+, et qui ajoute des instances de \verb+IntegerTabField+, \verb+DoubleTabField+ ou \verb+Point3dTabField+.
    Les boutons au centre permettent d'enregistrer, de voir l'image actuelle ou d'ajouter un objet.
    
    Le bouton «~Voir l'image~» fait appel à la classe \verb+GenerateImageAction+, qui importe le moteur de raytracing, et crée les images dans un autre thread, pour que l'interface ne soit pas bloquée.
    Elles sont alors affichées avec des JFrames, dans un JLabel.
    
    \begin{figure}[p]
      \centerline{\includegraphics[width=1.2\textwidth]{screen2.png}}
    \caption{Interface graphique\label{fig:gui}}
    \end{figure}
    \begin{figure}[p]
      \centerline{\includegraphics[width=1.2\textwidth]{guiuml.pdf}}
      \caption{Diagramme UML de la GUI\label{fig:guiuml}}
    \end{figure}

  \subsection{Raytracer}
    Pour créer le raytracer, nous avons convenu d'architecturer notre programme
    selon le diagramme de classes suivant présenté en figure \ref{fig:uml}.
    \begin{figure}[p]
      \centerline{\includegraphics[width=1.2\textwidth]{uml.pdf}}
      \caption{Diagramme UML\label{fig:uml}}
    \end{figure}

    \subsubsection{Changements de conception}
      Par rapport à ce que nous avions prévu lors de notre rapport d'analyse,
      nous avons fait quelques modifications: nous avons rajouté une classe
      pour représenter une lumière, que nous avions oubliée, nous n'utilisons
      pas la classe \verb+java.awt.color+ car il s'est avéré plus pratique de
      manipuler des \verb+double[]+ et nous n'utilisons pas
      \verb+javax.vectmath+, qui n'est pas installé à l'n7, mais nos
      propres classes.
      Ensuite, une scène peut posséder plusieurs caméras, ce qui permet d'avoir
      plusieurs images de la même scène de plusieurs points de vues différents.
      Nous avons ajouté les méthodes dont nous avons eu besoin à la classe
      \verb+Object+. Les classes \verb+Triangle+ et \verb+Plane+ héritent de
      \verb+TriPointed+ car ces objets ont beaucoup de points en commun: ils
      sont représentables par trois points et les formules qu'il y a derrière
      ces classes sont les mêmes; nous en avons profité pour ajouter une classe
      \verb+Parallelogram+ pour les mêmes raisons.
      
      La classe \verb+Mesh+ qui
      devait représenter un objet composé d'un ensemble de facettes
      triangulaires a été changée pour représenter un ensemble d'objets
      basiques, puisque nous n'utilisions rien de spécifique aux triangles la
      restriction n'était pas nécessaire, bien sûr un objet à facettes
      triangulaires peut être représenté par cette classe.
      
      Enfin nous avons ajouté une classe \verb+Texture+ pour alléger notre
      conception.

    \subsubsection{Fonctionnement}
      L'objet \verb+Scene+ dispose d'une méthode pour calculer la couleur
      d'un rayon passé en paramètre.
      Pour le faire, il va regarder quel objet va intersecter avec le rayon en
      premier, et appeler la méthode \verb+computeColor+ de l'objet en question.
      Cette méthode, définie dans la classe \verb+Object+, va faire tous les
      calculs nécessaires pour calculer les différentes composantes. Pour cela,
      elle peut même appeler à nouveau la méthode \verb+rayColor+ de la classe
      scène, sur les rayons réfléchis ou réfractés.
      Dans ces calculs, elle va appeler la méthode \verb+normal+, qui va donner
      la normale au point d'intersection du rayon avec l'objet, méthode qui sera
      définie dans des sous-classes, en fonction de l'objet.

      \begin{figure}[p]
        \centerline{
          \includegraphics[width=1.2\textwidth]{0377.png}
        }
        \caption{Exemple de rendu complexe}
      \end{figure}
      \begin{figure}[p]
        \centerline{
          \includegraphics[width=1.2\textwidth]{0699.png}
        }
        \caption{Exemple de rendu complexe, autre point de vue}
      \end{figure}

    \subsubsection{Problèmes}
    Un problème qui se pose est que, en lançant le rayon avec la méthode \verb+rayColor+ d'une scène,
    à partir d'un point d'intersection avec un objet, l'objet le plus proche qui va intersecter de nouveau
    sera l'objet lui même, à ce même point d'intersection et avec une distance de zéro.
    Pour cela, on aurait pu définir un $\varepsilon$ et vérifier que la distance y soit supérieure, mais
    cela entraînait des problèmes de pixels complètement différent des autres, et il aurait fallu trouver
    une valeur adéquate pour $\varepsilon$. Nous avons donc décidé de ne pas utiliser ce système, et tout
    simplement d'ignorer l'objet qui intersecte, lors du lancer de rayon.
    La méthode \verb+rayColor+ a donc un paramètre (pouvant être nul), qui indique si elle doit ignorer un objet.
    Ce système pose toutefois un problème: un rayon entrant dans une sphère coupera toutefois la sphère en un autre
    endroit. Ce cas est actuellement ignoré, et fait que notre réfraction dans une sphère est peu représentative de la réalité.
    Un autre problème assez important, mais lié à la façon dont le raytracer fonctionne, est qu'un objet transparent ou qui va seulement
    dévier légèrement la lumière, sera capable de masquer les sources lumineuses.
    Cela veut dire qu'il laissera une ombre, alors qu'il est pourtant transparent.

    On peut évaluer la complexité de notre algorithme à du
    $O(x\times y\times n\times m)$, où:
    \begin{itemize}
        \item $x$ est la largeur de l'image,
        \item $y$ est la hauteur de l'image,
        \item $n$ est le nombre d'objets dans l'image,
        \item $m$ est le nombre de sources lumineuses.
    \end{itemize}
    On voit donc que même si l'algorithme est rapide sur des scènes simples,
    plus la scène contiendra d'objets et de lumières, plus le temps de rendu
    sera élevé, même pour une scène simple, il n'est ainsi pas possible
    d'afficher le rendu en direct dans l'interface graphique.

    \subsubsection{Améliorations}
    Concernant le temps de rendu sur de grosses scènes, plusieurs choses sont possibles:
    \begin{itemize}
      \item on pourrait paralléliser la génération des images sur tous les
        cœurs disponibles sur l'ordinateur. C'est typiquement quelque chose de
        trivial à paralléliser puisque chaque pixel est généré indépendamment
        des autres (il faudrait cependant modifier nos objets qui conservent
        des données calculées entre les différents appels de méthodes);
      \item pour éviter de parcourir à chaque fois notre liste d'objets, on
        peut utiliser des techniques comme les partitions binaire de l'espace
        (BSP), ou les \textit{octree}. Ces techniques permettent de découper
        l'espace en plusieurs parties, et donc de ne parcourir qu'un morceau
        de la liste des objets.
      \item On pourrait aussi profiler le code, et essayer de voir si
        certaines méthodes ne gagneraient pas à être optimisées.
    \end{itemize}
    
\section{Améliorations supplémentaires}
Ayant fini le sujet à temps et désirant continuer à expérimenter avec notre raytracer, nous avons fait deux améliorations principales:
\begin{itemize}
    \item Un petit script nous a permis de générer de modifier un fichier représentant une scène, en déplaçant très légèrement la caméra.
              On peut donc obtenir une suite d'images, qui, passées à la suite dans une vidéo, permettent de «~filmer~» la scène en se déplaçant dedans.
              Vous pouvez voir une vidéo ici: \url{http://repo.palkeo.com/tmp/rendu1.mp4}.
              La vidéo est composée de 1000 images, avec 10 minutes de rendu par image. Pour éviter d'y passer une semaine, le rendu a été parallélisé sur les 200 ordinateurs de l'n7, et s'est fait en moins de deux heures.
    \item Nous avons aussi créé un objet «~ImageRect~», qui est capable d'appliquer une image existante sur un rectangle à l'intérieur de la scène.
\end{itemize}

\end{document}

